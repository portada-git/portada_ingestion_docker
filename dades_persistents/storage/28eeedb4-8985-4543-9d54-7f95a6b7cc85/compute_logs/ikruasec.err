2026-01-28 16:54:15 +0000 - dagster - DEBUG - ingestion - 28eeedb4-8985-4543-9d54-7f95a6b7cc85 - 1598 - LOGS_CAPTURED - Started capturing logs in process (pid: 1598).
2026-01-28 16:54:15 +0000 - dagster - DEBUG - ingestion - 28eeedb4-8985-4543-9d54-7f95a6b7cc85 - 1598 - raw_entries - STEP_START - Started execution of step "raw_entries".
2026-01-28 16:54:15 +0000 - dagster - DEBUG - ingestion - 28eeedb4-8985-4543-9d54-7f95a6b7cc85 - raw_entries - Loading file from: /opt/dagster/dagster_home/storage/ingested_entry_file using PickledObjectFilesystemIOManager...
2026-01-28 16:54:15 +0000 - dagster - DEBUG - ingestion - 28eeedb4-8985-4543-9d54-7f95a6b7cc85 - 1598 - raw_entries - LOADED_INPUT - Loaded input "data" using input manager "io_manager", from output "result" of step "ingested_entry_file"
2026-01-28 16:54:15 +0000 - dagster - DEBUG - ingestion - 28eeedb4-8985-4543-9d54-7f95a6b7cc85 - 1598 - raw_entries - STEP_INPUT - Got input "data" of type "Any". (Type check passed).
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
io.delta#delta-spark_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-a839713f-14dd-40aa-a988-8f90330c3ae8;1.0
	confs: [default]
	found io.delta#delta-spark_2.12;3.2.1 in central
	found io.delta#delta-storage;3.2.1 in central
	found org.antlr#antlr4-runtime;4.9.3 in central
:: resolution report :: resolve 97ms :: artifacts dl 2ms
	:: modules in use:
	io.delta#delta-spark_2.12;3.2.1 from central in [default]
	io.delta#delta-storage;3.2.1 from central in [default]
	org.antlr#antlr4-runtime;4.9.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-a839713f-14dd-40aa-a988-8f90330c3ae8
	confs: [default]
	0 artifacts copied, 3 already retrieved (0kB/3ms)
26/01/28 16:54:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[Stage 0:>                                                        (0 + 16) / 16]ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-28 16:54:34 +0000 - dagster - ERROR - ingestion - 28eeedb4-8985-4543-9d54-7f95a6b7cc85 - 1598 - raw_entries - STEP_FAILURE - Execution of step "raw_entries" failed.

dagster._core.errors.DagsterExecutionStepExecutionError: Error occurred while executing op "raw_entries"::

py4j.protocol.Py4JError: An error occurred while calling o74.save

Stack Trace:
  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/utils.py", line 57, in op_execution_error_boundary
    yield
  File "/usr/local/lib/python3.12/site-packages/dagster/_utils/__init__.py", line 394, in iterate_with_context
    next_output = next(iterator)
                  ^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/compute_generator.py", line 135, in _coerce_op_compute_fn_to_iterator
    result = invoke_compute_fn(
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/compute_generator.py", line 115, in invoke_compute_fn
    return fn(context, **args_to_pass) if context_arg_provided else fn(**args_to_pass)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/dagster_portada_project/assets/boat_fact_ingestion_assets.py", line 29, in raw_entries
    layer.save_raw_data(data=data, user=user)
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/portada_ingestion.py", line 525, in save_raw_data
    return super().save_raw_data(self.__container_path, user=user, data=data, source_path=source_path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/data_lake_metadata_manager.py", line 424, in wrapper
    return __is_data_transformer(func, "self", dataframe_key, description, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/data_lake_metadata_manager.py", line 118, in __is_data_transformer
    resultat = func(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/portada_ingestion.py", line 216, in save_raw_data
    start_counter = self.get_sequence_value("entry_ships", BoatFactDataModel(data_json_array[0])["publication_name"].lower(), increment=len(data_json_array))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/delta_data_layer.py", line 615, in get_sequence_value
    seq.write.format("delta").mode("overwrite").save(path)
  File "/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py", line 1463, in save
    self._jwrite.save(path)
  File "/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(

