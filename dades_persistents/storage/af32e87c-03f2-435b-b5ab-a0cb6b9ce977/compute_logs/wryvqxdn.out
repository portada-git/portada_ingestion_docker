config_path='/opt/dagster/app/data_layer_config/delta_data_layer_config.json' job_name='ingestion' py_spark_resource=PySparkResource(spark_config={'spark.sql.parquet.fs.optimized.committer.optimization-enabled': 'false', 'spark.sql.sources.commitProtocolClass': 'org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol', 'mapreduce.fileoutputcommitter.algorithm.version': '2', 'spark.sql.shuffle.partitions': '4', 'spark.databricks.delta.schema.autoMerge.enabled': 'true', 'spark.sql.caseSensitive': 'false', 'spark.sql.storeAssignmentPolicy': 'LEGACY', 'spark.sql.parquet.mergeSchema': 'true', 'spark.sql.legacy.allowUntypedScalaUDF': 'true', 'spark.jars.packages': 'io.delta:delta-spark_2.12:3.2.1', 'spark.sql.extensions': 'io.delta.sql.DeltaSparkSessionExtension', 'spark.sql.catalog.spark_catalog': 'org.apache.spark.sql.delta.catalog.DeltaCatalog', 'spark.master': 'local[*]', 'spark.app.name': 'DataLakePortadaCuration_01'})
