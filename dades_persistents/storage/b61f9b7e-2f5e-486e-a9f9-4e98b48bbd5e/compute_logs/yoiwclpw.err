2026-01-28 17:15:46 +0000 - dagster - DEBUG - ingestion - b61f9b7e-2f5e-486e-a9f9-4e98b48bbd5e - 1723 - LOGS_CAPTURED - Started capturing logs in process (pid: 1723).
2026-01-28 17:15:46 +0000 - dagster - DEBUG - ingestion - b61f9b7e-2f5e-486e-a9f9-4e98b48bbd5e - 1723 - raw_entries - STEP_START - Started execution of step "raw_entries".
2026-01-28 17:15:46 +0000 - dagster - DEBUG - ingestion - b61f9b7e-2f5e-486e-a9f9-4e98b48bbd5e - raw_entries - Loading file from: /opt/dagster/dagster_home/storage/ingested_entry_file using PickledObjectFilesystemIOManager...
2026-01-28 17:15:46 +0000 - dagster - DEBUG - ingestion - b61f9b7e-2f5e-486e-a9f9-4e98b48bbd5e - 1723 - raw_entries - LOADED_INPUT - Loaded input "data" using input manager "io_manager", from output "result" of step "ingested_entry_file"
2026-01-28 17:15:46 +0000 - dagster - DEBUG - ingestion - b61f9b7e-2f5e-486e-a9f9-4e98b48bbd5e - 1723 - raw_entries - STEP_INPUT - Got input "data" of type "Any". (Type check passed).
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
io.delta#delta-spark_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-84d17cd6-0efb-423d-b587-710796f04ed1;1.0
	confs: [default]
	found io.delta#delta-spark_2.12;3.2.1 in central
	found io.delta#delta-storage;3.2.1 in central
	found org.antlr#antlr4-runtime;4.9.3 in central
:: resolution report :: resolve 74ms :: artifacts dl 2ms
	:: modules in use:
	io.delta#delta-spark_2.12;3.2.1 from central in [default]
	io.delta#delta-storage;3.2.1 from central in [default]
	org.antlr#antlr4-runtime;4.9.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-84d17cd6-0efb-423d-b587-710796f04ed1
	confs: [default]
	0 artifacts copied, 3 already retrieved (0kB/2ms)
26/01/28 17:15:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
26/01/28 17:15:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2026-01-28 17:15:52 +0000 - dagster - ERROR - ingestion - b61f9b7e-2f5e-486e-a9f9-4e98b48bbd5e - 1723 - raw_entries - STEP_FAILURE - Execution of step "raw_entries" failed.

dagster._core.errors.DagsterExecutionStepExecutionError: Error occurred while executing op "raw_entries"::

ConnectionRefusedError: [Errno 111] Connection refused

Stack Trace:
  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/utils.py", line 57, in op_execution_error_boundary
    yield
  File "/usr/local/lib/python3.12/site-packages/dagster/_utils/__init__.py", line 394, in iterate_with_context
    next_output = next(iterator)
                  ^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/compute_generator.py", line 135, in _coerce_op_compute_fn_to_iterator
    result = invoke_compute_fn(
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/dagster/_core/execution/plan/compute_generator.py", line 115, in invoke_compute_fn
    return fn(context, **args_to_pass) if context_arg_provided else fn(**args_to_pass)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/dagster_portada_project/assets/boat_fact_ingestion_assets.py", line 29, in raw_entries
    layer.save_raw_data(data=data, user=user)
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/portada_ingestion.py", line 525, in save_raw_data
    return super().save_raw_data(self.__container_path, user=user, data=data, source_path=source_path, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/data_lake_metadata_manager.py", line 424, in wrapper
    return __is_data_transformer(func, "self", dataframe_key, description, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/data_lake_metadata_manager.py", line 118, in __is_data_transformer
    resultat = func(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/portada_ingestion.py", line 216, in save_raw_data
    start_counter = self.get_sequence_value("entry_ships", BoatFactDataModel(data_json_array[0])["publication_name"].lower(), increment=len(data_json_array))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/portada_data_layer/delta_data_layer.py", line 606, in get_sequence_value
    current_row = seq.first()
                  ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py", line 2997, in first
    return self.head()
           ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py", line 2973, in head
    rs = self.head(1)
         ^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py", line 2975, in head
    return self.take(n)
           ^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py", line 1407, in take
    return self.limit(num).collect()
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py", line 1262, in collect
    with SCCallSiteSync(self._sc):
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/traceback_utils.py", line 81, in __exit__
    self._context._jsc.setCallSite(None)
  File "/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1321, in __call__
    answer = self.gateway_client.send_command(command)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "/usr/local/lib/python3.12/site-packages/py4j/clientserver.py", line 438, in connect_to_java_server
    self.socket.connect((self.java_address, self.java_port))

The above exception occurred during handling of the following exception:
py4j.protocol.Py4JError: An error occurred while calling o57.collectToPython

Stack Trace:
  File "/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py", line 1263, in collect
    sock_info = self._jdf.collectToPython()
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/site-packages/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(

